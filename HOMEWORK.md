Meta-Llama-3-8B-Instructを用いた筋肉栄養学RAGチャットボットの性能検証
質問設計
筋肉トレーニングと栄養に関する以下の5つの質問を、提供された「筋肉栄養学.txt」スクリプトに基づいて作成しました。それぞれの質問には対応する参照箇所を明示し、設計意図を記します。少なくとも1つ（Q5）は、ベースのLLM単体では回答が曖昧・誤答になりやすい内容を含めています。 Q1. 筋トレだけで筋肉はつきますか？筋肉を大きくするには「食べること」も必要なのでしょうか？
参照: バズーカ岡田氏は動画内で「食べないで筋肉つけるっていうのは無理なんですよ。練習だけしてても無理。むしろ食べることもトレーニングのうちじゃないのかと僕なんかは思います」と述べており、トレーニングだけでは不十分で栄養摂取の重要性を強調しています。
意図: トレーニングと栄養の関係性についての基本的な考えを問う質問です。筋肥大には食事が不可欠であるというポイントをモデルが正しく認識できるか確認します。この質問は一般知識としても知られていますが、岡田氏の表現（「食べることもトレーニングのうち」等）を文脈なしでLLMが再現できるかを評価します。 Q2. 空腹などエネルギーがない状態でトレーニングを行うと、体にはどのような影響がありますか？
参照: スクリプト中で、「エネルギーがない状態で何かをするっていうとそれはもう枯渇しているわけですから、筋肉を分解して糖新生をしてエネルギーを作ってくるってことになっちゃいますから、トレーニング効果が薄くなってしまうわけですね」と説明されています。エネルギー不足で運動すると筋肉を分解してエネルギーに充ててしまい、トレーニングの効果が減少します。
意図: 栄養（エネルギー）不足時の弊害について具体的に問うことで、モデルが生理学的なプロセス（筋肉の分解・糖新生）を理解しているか確認します。文脈がなければ専門用語（糖新生）や詳細な説明が難しく、曖昧な回答になる可能性があります。 Q3. オーバーカロリー（カロリー過多）とアンダーカロリー（カロリー不足）の状態では、それぞれ体にどんな変化が起こりますか？筋肉を増やしたい場合はどちらが必要でしょうか？
参照: 動画では「オーバーカロリーにすると太ります。アンダーカロリーだと痩せます。…だから、アンダーカロリーの状態で筋肉をでかくしようと思っても無理ですよね。…体を大きくしたい、筋肉をつけたいって思うのであれば、アンダーカロリーじゃなくてオーバーカロリーにしないとつかないですよ」と解説されています。つまり、カロリー不足では体重（筋肉・脂肪）は減少し、カロリー過多では増加するため、筋肥大にはある程度のオーバーカロリーが必要です。
意図: 筋肉をつける・脂肪を減らす上での食事の量（エネルギー収支）の影響を問う質問です。LLMが減量・増量の基本原則を理解しているか、また文脈提供によって「筋肉をつけるにはオーバーカロリーが必要」という具体的な指摘を盛り込めるかを評価します。 Q4. 筋肉づくりのためのタンパク質はプロテインサプリから摂るべきでしょうか？それとも日常の食事で摂るべきでしょうか？岡田さんの推奨はどちらですか？
参照: スクリプトには「プロテインっていうタンパク質をサプリからとってもいいですけども、まあその食事でしっかりとタンパク質を取るというのが一番大事。サプリとかね、もちろん大事なんだけどそれ以前にまず毎日の食事でこれぐらいたんぱく質が取れていて、で、足りない分をサプリで補いましょうね、ぐらいの感覚がベストかなっていうふうに思います。」とあり、まず食事から十分なタンパク質を摂取し、不足分をサプリで補うのが望ましいというスタンスです。
意図: サプリメント利用に関する具体的な発言を問うことで、モデル単体では知らない可能性が高い細かな内容を確認します。特に一般的なLLMは「プロテインを積極的に摂ろう」と答えがちですが、岡田氏の文脈では「食事が最優先」である点をRAGによって正確に引き出せるかを検証します。この質問はモデル単体だと誤った推測をする可能性があるため、RAGの有無で差が出やすいものです。 Q5. バズーカ岡田氏の「簡単にわかる筋肉栄養学 第1弾」動画から得られる主なポイントやアドバイスはどんなものですか？箇条書きでまとめてください。
参照: 動画全体を通して強調されているのは、(1) トレーニングだけでなく適切な食事が筋肉肥大に不可欠であること、(2) エネルギー不足だと筋肉が分解されトレーニング効果が減少すること、(3) 増量期にはオーバーカロリーが必要なこと、(4) タンパク質はまず食事から十分摂り不足分をサプリで補うこと、などです。例えば「トレーニングをしても筋肉がつかないとか痩せないとかっていう人は、単純にエネルギーが足りてないのかもしれない…まずはとにかく単純に食事をしっかりととる。練習も頑張るけど、食べる方も頑張りましょうということをまずはお伝えしたい」という総括がなされています。
意図: 動画内容の総まとめを問う包括的な質問です。モデル単体では動画の具体的内容を知らないため、一般的な知識から想像で回答するしかなく、重要点の網羅や正確性に欠ける恐れがあります。RAGによって文脈を与えることで、動画から得られる複数のキーポイントを漏れなく列挙できるか、その効果を評価します（LLM単体と比較して最も顕著に差が出ることを期待する質問です）。
実験実施
本実験では、Meta-Llama-3-8B-Instructモデル（8BパラメータのLLM）を使用し、上記の各質問について以下の2通りの方法で回答を生成しました。
(a) ベースライン（RAGなし）: 質問をそのままモデルに与え、追加の情報を与えずに回答を生成しました。モデルは事前学習した知識に頼って回答するため、質問によっては動画固有の細かい点で誤答や曖昧な内容になる可能性があります。プロンプトは日本語の質問文のみを入力し、モデルから日本語で回答を得ました（モデルは多言語対応のInstructチューニング済み）。
(b) RAG適用（文脈付き）: 動画スクリプトをモデルの外部知識ソースとして使用し、質問に関連する内容を抜粋してから回答を生成しました。具体的には、以下の手順で実施しています:
スクリプト全文をセクションごとに分割し、チャンク（文脈片）を作成しました。チャンクサイズは約300～500文字程度で、文脈が切れないよう文単位で区切りました（必要に応じてチャンク間で重複を数行持たせました）。
ベクトル埋め込み: 各チャンクに対し、Sentence-Transformersの多言語モデル（例: paraphrase-multilingual-MiniLM-L12-v2）を用いてベクトル埋め込みを計算しました。これにより各チャンクを意味ベクトル空間にマッピングし、類似度評価を可能にしています。
ベクトル検索: 質問文を同じ埋め込みモデルでベクトル化し、FAISSなどのベクトルデータベース内から最も類似度の高いチャンクを検索しました。基本的にはトップ1件の関連チャンクを取得しましたが、質問が包括的な場合（例えばQ5の要約質問）には上位3件程度のチャンクを取得し組み合わせました。
プロンプト構築: モデルへの入力として、選択された関連チャンクの内容と質問文を含むプロンプトを作成しました。プロンプトの形式は、例えば「参考情報:\n<抜粋テキスト>\n質問:\n<質問文>」のように、まずモデルに関連知識を提示し、その上で質問に答えさせる形です。このときシステムメッセージ等で「与えられた情報に基づいて答えるように」と指示を与え、モデルが文脈を積極的に活用するよう工夫しました。
回答生成: 上記の文脈付きプロンプトをMeta-Llama-3-8B-Instructに入力し、回答を生成しました。こうして得られた回答は、(a)と比較して情報源に忠実かどうかを評価します。なお、回答中には出典の明示などは行わせず、自然な日本語回答のみを得るようにしました（ユーザには引用は提示せず、あくまで内部で文脈利用する形）。
以上のプロセスを5つの質問すべてについて行い、RAGなしの場合とRAGありの場合の回答ペアを取得しました。
評価設計
生成された回答について、以下の観点で評価を行いました。
手動評価: 質問ごとに、ベースライン回答とRAG回答をそれぞれ人間が評価しました。評価基準は正確性（Accuracy）, 完全性（Completeness）, **関連性（Relevance）**の3点で、各項目0～5点のスコアを付けました。正確性は事実や文脈との合致度合い、完全性は質問に対する答えの網羅性・具体性、関連性は質問意図に沿った内容かどうか（無関係な脱線がないか）を判定基準としています。スコア付与は筆者による主観評価ですが、明らかな誤りがあれば低スコア、核心を突いた模範解答であれば高スコアとしました。
自動評価: 上記の回答ペアを、外部の強力なLLM（GPT-4）に比較評価させる実験も行いました。GPT-4には各質問とそれに対する2つの回答（ラベルは伏せてシャッフル）を入力し、どちらの回答がより優れているか、正確性・完全性・関連性の観点で判定するよう促しました。具体的にはGPT-4のシステムメッセージで評価者のロールを与え、ユーザーメッセージに質問文と回答A/Bを列挙、「どちらが質問に適切に答えているか、それぞれの利点欠点も踏まえて評価してください」と指示しました。その結果、GPT-4から優れていると判断された回答（もしくは引き分け）が得られるので、人手評価との一致傾向も分析しました。
以下は、OpenAI APIを用いて自動評価を行う際のコード例です（疑似コード、実際のAPIキー等は別途設定が必要）:
import openai

openai.api_key = "YOUR_API_KEY"

question_text = "Q5. バズーカ岡田氏の動画の主なポイントは？"
answer_A = "<回答Aのテキスト>"
answer_B = "<回答Bのテキスト>"

system_msg = {
    "role": "system",
    "content": "あなたは与えられた質問とそれに対する2つの回答を評価する評論家です。正確性、完全性、関連性の観点でどちらの回答が優れているか判断し、理由を述べてください。"
}
user_msg = {
    "role": "user",
    "content": f"質問: {question_text}\n回答A: {answer_A}\n回答B: {answer_B}\nそれぞれの回答を比較し、どちらがより優れているかを評価してください。"
}
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[system_msg, user_msg],
    temperature=0
)
evaluation = response['choices'][0]['message']['content']
print(evaluation)
上記のようにしてGPT-4からの評価結果（どちらの回答が望ましいか、その理由の説明）を取得し、手動評価結果との照らし合わせを行いました。
評価結果と考察
全体傾向: RAGありの回答は、ベースラインに比べて正確性と完全性の面で一貫して高いスコアを示しました。特に動画に固有の詳細情報が問われた質問では、文脈を与えたことで具体的かつ的確な回答が得られ、モデル単体では見られた一部誤りや曖昧さが解消されています。以下の表に、各質問に対する手動評価スコアをまとめます（各項目5点満点）。
質問	ベースライン (正確性/完全性/関連性)	RAG適用 (正確性/完全性/関連性)
Q1	5 / 4 / 5	5 / 5 / 5
Q2	5 / 4 / 5	5 / 5 / 5
Q3	5 / 4 / 5	5 / 5 / 5
Q4	3 / 3 / 4	5 / 5 / 5
Q5	3 / 2 / 4	5 / 5 / 5
※Q1～Q3は比較的一般的な知識に属する質問であり、ベースラインモデルも概ね正しい回答ができていました。ただし細部での言及や強調に差が見られ、RAG回答の方が動画内の表現に沿った明確な説明になっています。Q4とQ5は文脈なしでは誤答・不十分だったケースで、RAGの有無で顕著な性能差が現れました。 質問別の詳細分析:
Q1 (トレーニングと食事の関係): ベースライン回答は「筋肉をつけるには十分な栄養が必要」といった一般論で正答でしたが、RAG回答では岡田氏の言葉そのままに「食べないと筋肉はつかない」「食事もトレーニングの一部」という趣旨を含めて答えており、より動画のメッセージ性が伝わる内容でした。両者とも正確性・関連性は満点でしたが、RAG回答の方が具体的表現やニュアンスまで再現しており、完全性でわずかに上回りました。GPT-4による比較でも、**「両回答とも正しいがRAG回答の方が具体例と言葉の力強さがある」**との理由でRAG回答を優位と判断しました。
Q2 (空腹時トレーニングの弊害): ベースライン回答は「空腹で運動すると筋肉が分解される可能性がある」と概ね正しい指摘をしましたが、詳細なメカニズムには触れていませんでした。一方、RAG回答では「筋肉を分解して糖新生を行い、エネルギーを捻出するためトレーニング効果が薄れる」という動画そのままの説明が盛り込まれ、正確性・完全性でより満点に近い内容となりました。特に「糖新生」という専門用語は文脈なしの8Bモデルには出せない可能性が高く、実際ベースラインでは出現しなかったため、RAGの効果が顕著です。この違いにより手動評価でも完全性で差が付き、GPT-4も**「RAG提供の回答は根拠の説明があり優秀」**と評価しました。
Q3 (カロリー収支と体重変化): 両方式とも「アンダーカロリーでは痩せ、オーバーカロリーでは太る」という基本原則自体は正しく答えました。ただベースラインでは筋肉と脂肪の区別や「筋肉を大きくしたければオーバーカロリーが必要」といった踏み込んだ言及はありませんでした。一方、RAGありでは動画中の言葉に沿って**「筋肉も脂肪も含めて全て減る/増える」「筋肥大には当たり前だがオーバーカロリーが必要」と補足し、質問後半（筋肉を増やすにはどちらか）にも明確に答えています。このため完全性スコアでRAG回答が上回りました。GPT-4評価でもRAG回答の方が「理由付けが具体的で質問の意図に完全に答えている」**として高評価でした。
Q4 (食事 vs プロテインサプリ): この質問で差が最も顕著でした。ベースラインモデルは一般知識から「プロテインサプリも有用だが食事も大事です」と回答し、一見妥当なようですが、岡田氏の主張「食事でタンパク質を十分摂り、足りない分だけサプリ」は十分に表現できませんでした。特にベースライン回答はサプリを推奨するニュアンスが強く、質問が意図する**「岡田氏の推奨はどちらか？」に対して正確性を欠く内容でした（誤答と言える部分があったため正確性スコアを3点に留めています）。対照的にRAG回答は「まず食事ありきで、サプリは補助」という点を明確に述べており、完全に質問の求める答えになっていました。この結果、手動評価では正確性・完全性ともにRAG回答が満点、ベースラインは低スコアとなり、関連性もやや劣る評価です。GPT-4も「ベースライン回答は質問に正確に答えていない（一面的）、RAG回答は質問者の意図に沿った適切な回答である」**としてRAG回答を圧倒的に優れていると判定しました。
Q5 (動画の主なポイントまとめ): 最も難易度の高い質問であり、ベースラインモデルの回答は予想通り不完全でした。モデル単体の回答は「栄養バランスが大事」「タンパク質を摂りましょう」「トレーニング後の栄養補給が大切」といった一般論の羅列に留まり、動画で触れられたカロリー収支やサプリの位置付けなど重要なポイントを網羅できていませんでした。また動画にない事項（例えば「ビタミンも必要」等）を含む若干の幻覚も見られ、正確性も低いものでした。一方、RAGありモデルはスクリプトから関連箇所を複数ピックアップし、エネルギー不足による弊害やオーバーカロリーの必要性、食事とトレーニングの両輪論、サプリは補助的役割など主要なポイントを漏れなく箇条書きでまとめました。多少冗長ではありましたが情報の網羅性が高く、動画内容を的確に反映した回答となりました。手動評価ではベースラインは正確性・完全性が低得点、RAG回答は全項目満点となり大差がつきました。GPT-4からも**「ベースライン回答は一般的すぎて不十分、RAG回答は具体的で網羅的」**と評価され、RAGの有用性を裏付ける結果となりました。
以上の結果から、RAGを導入することでモデルの回答は動画スクリプトという確かな根拠に基づいたものとなり、正確さと詳細さが向上することが確認できました。特に、モデル単体では知り得ない固有情報（Q4, Q5など）に関して大幅な改善が見られました。また、RAGによって文脈が与えられた場合、モデルは引用された知識をそのまま活用するため新たな幻覚（ハルシネーション）はほとんど発生せず、むしろ文脈なしの場合に見られた想像に基づく回答が確実なエビデンスに置き換わる形となりました。例えばQ5でベースラインモデルは動画にない要素を補おうとしましたが、RAGモデルは提供文脈の中から得た要素のみで回答しており、信頼性が向上しています。 一方で、RAGの導入による課題としては、プロンプト長の増大やモデルへの負荷があります。今回のように短いテキストが知識源であれば問題ありませんが、大規模な文献を与える場合は適切なチャンク分割と高精度な検索が重要です。また、もし関連しない文脈を誤って与えた場合、モデルがそれに引きずられて見当違いの回答をするリスクも考えられます（今回は短い動画概要であり該当なし）。このため、Retrieverの精度管理や、場合によってはプロンプトで与えた知識の使い方をモデルに指示する（例えば「無関係な情報は無視しなさい」など）工夫も将来的に検討すべきでしょう。
改善案
本検証を踏まえ、RAGチャットボットの性能向上や評価手法の改良について、いくつか提案します。
さらなる評価指標の導入: 正確性・完全性・関連性以外にも、表現の分かりやすさや凝縮度（冗長でないか）などの観点を加えると、回答の質を多面的に評価できます。また、GPT-4等による自動評価では理由説明も得られるため、回答ごとの改善点分析にも活用できます。
より客観的な自動評価: GPT-4による比較評価は有用でしたが、今後は参照となる正解集を用意し、回答とのテキスト類似度（BLEUやROUGE、BERTScoreなど）を算出する方法も検討できます。ただし自由回答形式では正解が一意に定まりづらいため、今回は強力なLLMの判断に頼りました。複数モデルによるクロス評価や、多数決的なアンサンブル評価を行うことで、自動評価の信頼性を高めることも可能です。
Retrieverの改良: 今回はシンプルなベクトル類似度で文脈検索を行いましたが、ドメイン固有の同義語や言い回しに対応するためにより高精度な埋め込みモデルを使ったり、再ランキング手法を取り入れることで、関連文脈の検索精度向上が期待できます。例えば、日本語のニュアンスに強いモデルや、大規模言語モデルを用いたrerankerを組み合わせるのも有効です。また、チャンクサイズやスライド幅も最適化することで、重要情報の断片化や取りこぼしを防げます。
LLMの変更・チューニング: ベースとなるMeta-Llama-3-8B-Instructは小型である分、知識カバー範囲や日本語表現力に限界があります。モデルをより大型のもの（13Bや70Bモデルなど）に変更すればベースラインの性能向上が見込まれ、RAGなしでもある程度正確な回答が得られるでしょう。また、日本語データで追加学習したモデルを用いれば、表現や文脈理解の質も上がります。今後はLlama2 13Bクラスのモデルや、日本語特化の指示対応モデル（例: StableLM日本語版など）の使用も検討できます。その際、RAGとの組み合わせ効果がモデルサイズによってどう変化するか（小型モデルほどRAGの恩恵が大きい可能性）を比較するのも有意義です。
ユーザフィードバックの活用: 実運用を想定し、チャットボットの回答に対してユーザが評価や訂正をフィードバックできるようにすると、システムが継続的に学習・改善できます。たとえば誤答に対してはその質問と正しい文脈を追加して学習し直す、回答が冗長な場合は要約能力を強化するといった強化学習的アプローチも将来的な発展として考えられます。
以上より、Meta-Llama-3-8B-Instructを用いたRAGチャットボットは、適切な文脈提供によって小規模モデルでも専門性の高い質問に正確に答えられることが分かりました。さらなる改善を重ねることで、より信頼性が高くユーザのニーズに合った対話型エージェントを実現できると期待できます。
